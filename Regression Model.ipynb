{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a regression model on this problem:\n",
    "Now the question is \"How many more cycles will the machine last before it fail?\" instead of \"Will the machine fail within certain number of cycles?\"\n",
    "\n",
    "The code is largely the same except when building the model. More specifically, the difference is the activation function on the final dense layer. Now is linear instead of sigmoid in the classification model. And some other changes as well, such as the optimization algorithm is RMSprop and loss function is MSE.\n",
    "\n",
    "Below is the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "model_path = ''\n",
    "\n",
    "# function for sort the csv file in numerical order when entering for loop\n",
    "# to ensure the ID value is correctly assigned\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "def filter(df):\n",
    "    low = .01\n",
    "    high = .99\n",
    "    quant_df = df.quantile([low, high])\n",
    "    fil_df = df.apply(lambda x: x[(x>=quant_df.loc[low,x.name]) \n",
    "                                                & (x <= quant_df.loc[high,x.name])], axis=0)\n",
    "    return fil_df\n",
    "\n",
    "##########################\n",
    "## Data Preprocessing  ##\n",
    "##########################\n",
    "\n",
    "dfTrain_List = []\n",
    "ID = 0\n",
    "\n",
    "for train_file in sorted(glob.glob('smalldata/train/*_rms.csv'), key=numericalSort):\n",
    "    trainDf = pd.read_csv(train_file)\n",
    "    trainDf.drop('timestamp', axis=1, inplace=True)\n",
    "    trainDf = filter(trainDf)\n",
    "    # fill NaN with first interpolate then fillna function\n",
    "    trainDf = trainDf.interpolate()\n",
    "    trainDf = trainDf.fillna(0)\n",
    "    # assign id to each machine\n",
    "    trainDf[\"id\"] = ID\n",
    "    ID += 1\n",
    "    # numbering each monitor cycle\n",
    "    trainDf[\"cycle\"] = trainDf.index + 1\n",
    "    # Tof: Time to failure\n",
    "    trainDf[\"TTF\"] = trainDf[\"cycle\"].values[::-1]\n",
    "    # add all training data to a list\n",
    "    dfTrain_List.append(trainDf)\n",
    "    \n",
    "\n",
    "# combine all training data together    \n",
    "concatTrain = pd.concat(dfTrain_List, axis=0)\n",
    "\n",
    "# generate label columns for training data\n",
    "# is a machine going to fail within x cycles?\n",
    "# every day monitors will run ~150 cycles\n",
    "# we can set x to any numbers, but I set it as 150 to see whether a machine will fail within a day\n",
    "\n",
    "x = 150\n",
    "concatTrain['label'] = np.where(concatTrain['TTF'] <= x, 1, 0 )\n",
    "\n",
    "# Training data MinMax normalization \n",
    "concatTrain['cycle_norm'] = concatTrain['cycle']\n",
    "\n",
    "# df.columns.difference returns as output a new list of columns \n",
    "# from the existing columns excluding the ones given as arguments.\n",
    "# separate out the cols that need to be normalized as cols_normalized\n",
    "cols_normalize = concatTrain.columns.difference(['id','cycle','TTF','label'])\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(concatTrain[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=concatTrain.index)\n",
    "\n",
    "# combine normalized data columns with non-normalized colums\n",
    "join_df = pd.concat([concatTrain[concatTrain.columns.difference(cols_normalize)], norm_train_df], axis=1)\n",
    "concatTrain = join_df.reindex(columns = concatTrain.columns)\n",
    "\n",
    "\n",
    "# same preprocessing to test data\n",
    "dfTest_List = []\n",
    "\n",
    "for test_file in sorted(glob.glob('data/test/*_rms.csv'), key=numericalSort):\n",
    "    testDf = pd.read_csv(test_file)\n",
    "    testDf.drop('timestamp', axis=1, inplace=True)\n",
    "    testDf = filter(testDf)\n",
    "    # fill NaN with first interpolate then fillna function\n",
    "    testDf = testDf.interpolate()\n",
    "    testDf = testDf.fillna(0)\n",
    "    testDf[\"id\"] = ID\n",
    "    ID += 1\n",
    "    testDf[\"cycle\"] = testDf.index + 1\n",
    "    testDf[\"TTF\"] = testDf[\"cycle\"].values[::-1]\n",
    "\n",
    "    dfTest_List.append(testDf)\n",
    "    \n",
    "concatTest = pd.concat(dfTest_List, axis=0)\n",
    "\n",
    "concatTest['cycle_norm'] = concatTest['cycle']\n",
    "\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.fit_transform(concatTest[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=concatTest.index)\n",
    "\n",
    "join_df = pd.concat([concatTest[concatTest.columns.difference(cols_normalize)], norm_test_df], axis=1)\n",
    "\n",
    "concatTest = join_df.reindex(columns = concatTest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##  Generate data sequence  ##\n",
    "##############################\n",
    "\n",
    "sequence_length = 150\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), \n",
    "                           range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "sequence_cols = ['rpm', 'motor_voltage', 'motor_current', 'motor_temp', \n",
    "                 'inlet_temp', 'cycle_norm']\n",
    "\n",
    "seq_gen = (list(gen_sequence(concatTrain[concatTrain['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in concatTrain['id'].unique())\n",
    "\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "label_gen = [gen_labels(concatTrain[concatTrain['id']==id], sequence_length, ['TTF']) \n",
    "             for id in concatTrain['id'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Model Summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 150, 100)          42800     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 150, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 73,051\n",
      "Trainable params: 73,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 86561 samples, validate on 4556 samples\n",
      "Epoch 1/1\n",
      "86561/86561 [==============================] - 122s 1ms/step - loss: 2940217866.3215 - mean_absolute_error: 48062.3949 - r2_keras: -3.6696 - val_loss: 6947931.4160 - val_mean_absolute_error: 2280.3213 - val_r2_keras: -20.0213\n",
      "91117/91117 [==============================] - 40s 441us/step\n",
      "Accurracy: 45770.72972017579\n",
      "91117/91117 [==============================] - 39s 433us/step\n",
      "Confusion matrix\n",
      "- x-axis is true labels.\n",
      "- y-axis is predicted labels\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "(30, 150, 6)\n",
      "PREDIECTED TEST remaining cycles:\n",
      "[[11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934982]\n",
      " [11.934981]\n",
      " [11.934982]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934982]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934982]\n",
      " [11.934983]\n",
      " [11.934982]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]\n",
      " [11.934983]]\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "##  Model training and testing  ##\n",
    "##################################\n",
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination \n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae', r2_keras])\n",
    "\n",
    "\n",
    "print(\"Regression Model Summary:\")\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(seq_array, label_array, epochs=1, batch_size=2000, validation_split=0.05, verbose=1,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, \n",
    "                                                     verbose=0, mode='min')])\n",
    "\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('Accurracy: {}'.format(scores[1]))\n",
    "\n",
    "y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_true = label_array\n",
    "\n",
    "\n",
    "seq_array_test_last = [concatTest[concatTest['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in concatTest['id'].unique() \n",
    "                       if len(concatTest[concatTest['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "\n",
    "print(seq_array_test_last.shape)\n",
    "\n",
    "# make predictions and compute confusion matrix\n",
    "\n",
    "# if os.path.isfile(model_path):\n",
    "#     estimator = load_model(model_path, custom_objects={'r2_keras': r2_keras})\n",
    "y_pred_test = model.predict(seq_array_test_last)\n",
    "\n",
    "print(\"PREDIECTED TEST remaining cycles:\")\n",
    "# print(y_pred_test.shape())\n",
    "print(y_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
